{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generating Harry Potter Story Using RNN and Tensorflow.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPqrSJkSVWqseQFnKRyQdH/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikhilrathaur/Generating-Harry-Potter-Story-Using-RNN-and-Tensorflow/blob/master/Generating_Harry_Potter_Story_Using_RNN_and_Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lwl2u8LbD-s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "4dd7af1e-7479-44dc-c154-bd9e24fbb1e3"
      },
      "source": [
        "!gdown 'https://drive.google.com/uc?id=1gpfiJNP0A7r6z_LGUENv6dFWGnMPl0Ga'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1gpfiJNP0A7r6z_LGUENv6dFWGnMPl0Ga\n",
            "To: /content/144820_338363_bundle_archive.zip\n",
            "\r0.00B [00:00, ?B/s]\r2.39MB [00:00, 76.6MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOzCpaCdbsG6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "10648d84-9d83-460c-dccc-63ef6d8d7afa"
      },
      "source": [
        "!unzip '/content/144820_338363_bundle_archive.zip'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/144820_338363_bundle_archive.zip\n",
            "  inflating: 1SorcerersStone.txt     \n",
            "  inflating: 2ChamberofSecrets.txt   \n",
            "  inflating: 3ThePrisonerOfAzkaban.txt  \n",
            "  inflating: 4TheGobletOfFire.txt    \n",
            "  inflating: 5OrderofthePhoenix.txt  \n",
            "  inflating: 6TheHalfBloodPrince.txt  \n",
            "  inflating: 7DeathlyHollows.txt     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2sWKLlAb6DY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XA23eSJocBJf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files= ['1SorcerersStone.txt', '2ChamberofSecrets.txt', '3ThePrisonerOfAzkaban.txt', '4TheGobletOfFire.txt', '5OrderofthePhoenix.txt', '6TheHalfBloodPrince.txt', '7DeathlyHollows.txt']\n",
        "with open('harrypotter.txt', 'w') as outfile:\n",
        "  for file in files:\n",
        "    with open(file) as infile:\n",
        "      outfile.write(infile.read())\n",
        "  text = open('harrypotter.txt').read()"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7-WJI_-crWL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "153bdc12-8454-420d-8e28-632ef5a2c659"
      },
      "source": [
        "print(text[:300])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Harry Potter and the Sorcerer's Stone \n",
            "\n",
            "CHAPTER ONE \n",
            "\n",
            "THE BOY WHO LIVED \n",
            "\n",
            "Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you'd expect to be involved in anything strange or mysterious, because they \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6jmmHlxctX2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aa9d4984-300e-4778-b788-85ab1b144ab1"
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "char2index = {u:i for i, u in enumerate(vocab)}\n",
        "index2char = np.array(vocab)\n",
        "text_as_int = np.array([char2index[c] for c in text])\n",
        "#how it looks:\n",
        "print ('{} -- characters mapped to int -- > {}'.format(repr(text[:13]), text_as_int[:13]))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Harry Potter ' -- characters mapped to int -- > [39 64 81 81 88  3 47 78 83 83 68 81  3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_6i0s6odGjE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "def split_input_target(data):\n",
        "  input_text = data[:-1]\n",
        "  target_text = data[1:]\n",
        "  return input_text, target_text\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLmTtNOOdPN2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CVEXkfzdYWA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "embedding_dim = 300\n",
        "# Number of RNN units \n",
        "rnn_units1 = 512\n",
        "rnn_units2 = 256\n",
        "rnn_units= [rnn_units1, rnn_units2]\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "       batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.GRU(rnn_units1, return_sequences=True,\n",
        "       stateful=True,recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.GRU(rnn_units2, return_sequences=True,\n",
        "       stateful=True,recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "model = build_model(\n",
        "vocab_size = vocab_size,\n",
        "embedding_dim=embedding_dim,\n",
        "rnn_units=rnn_units,\n",
        "batch_size=BATCH_SIZE)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCMlfrCIdar_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels,\n",
        "         logits, from_logits=True)\n",
        "model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckkj5MxHdgGe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "   filepath=checkpoint_prefix, save_weights_only=True)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufKvtMggdlR3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e26fb887-d0db-4888-8d97-15283efd4862"
      },
      "source": [
        "EPOCHS= 50\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
        "latest_check = tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "967/967 [==============================] - 38s 39ms/step - loss: 1.7533 - accuracy: 0.4992\n",
            "Epoch 2/50\n",
            "967/967 [==============================] - 39s 40ms/step - loss: 1.2926 - accuracy: 0.6140\n",
            "Epoch 3/50\n",
            "967/967 [==============================] - 39s 40ms/step - loss: 1.2228 - accuracy: 0.6319\n",
            "Epoch 4/50\n",
            "967/967 [==============================] - 39s 40ms/step - loss: 1.1880 - accuracy: 0.6412\n",
            "Epoch 5/50\n",
            "967/967 [==============================] - 39s 40ms/step - loss: 1.1653 - accuracy: 0.6473\n",
            "Epoch 6/50\n",
            "967/967 [==============================] - 39s 40ms/step - loss: 1.1493 - accuracy: 0.6519\n",
            "Epoch 7/50\n",
            "967/967 [==============================] - 39s 40ms/step - loss: 1.1365 - accuracy: 0.6553\n",
            "Epoch 8/50\n",
            "967/967 [==============================] - 39s 40ms/step - loss: 1.1262 - accuracy: 0.6581\n",
            "Epoch 9/50\n",
            "967/967 [==============================] - 39s 40ms/step - loss: 1.1180 - accuracy: 0.6604\n",
            "Epoch 10/50\n",
            "967/967 [==============================] - 39s 40ms/step - loss: 1.1108 - accuracy: 0.6624\n",
            "Epoch 11/50\n",
            "967/967 [==============================] - 39s 40ms/step - loss: 1.1052 - accuracy: 0.6641\n",
            "Epoch 12/50\n",
            "967/967 [==============================] - 39s 40ms/step - loss: 1.0997 - accuracy: 0.6656\n",
            "Epoch 13/50\n",
            "967/967 [==============================] - 39s 40ms/step - loss: 1.0958 - accuracy: 0.6666\n",
            "Epoch 14/50\n",
            "967/967 [==============================] - 39s 40ms/step - loss: 1.0919 - accuracy: 0.6677\n",
            "Epoch 15/50\n",
            "967/967 [==============================] - 39s 40ms/step - loss: 1.0885 - accuracy: 0.6687\n",
            "Epoch 16/50\n",
            "967/967 [==============================] - 39s 40ms/step - loss: 1.0855 - accuracy: 0.6694\n",
            "Epoch 17/50\n",
            "967/967 [==============================] - 39s 40ms/step - loss: 1.0829 - accuracy: 0.6702\n",
            "Epoch 18/50\n",
            "967/967 [==============================] - 39s 40ms/step - loss: 1.0805 - accuracy: 0.6709\n",
            "Epoch 19/50\n",
            "967/967 [==============================] - 39s 40ms/step - loss: 1.0788 - accuracy: 0.6714\n",
            "Epoch 20/50\n",
            "967/967 [==============================] - 39s 40ms/step - loss: 1.0770 - accuracy: 0.6717\n",
            "Epoch 21/50\n",
            "967/967 [==============================] - 39s 40ms/step - loss: 1.0751 - accuracy: 0.6724\n",
            "Epoch 22/50\n",
            "967/967 [==============================] - 39s 40ms/step - loss: 1.0738 - accuracy: 0.6727\n",
            "Epoch 23/50\n",
            "967/967 [==============================] - 39s 40ms/step - loss: 1.0725 - accuracy: 0.6730\n",
            "Epoch 24/50\n",
            "967/967 [==============================] - 39s 40ms/step - loss: 1.0713 - accuracy: 0.6734\n",
            "Epoch 25/50\n",
            "967/967 [==============================] - 39s 40ms/step - loss: 1.0698 - accuracy: 0.6737\n",
            "Epoch 26/50\n",
            "967/967 [==============================] - 39s 40ms/step - loss: 1.0688 - accuracy: 0.6740\n",
            "Epoch 27/50\n",
            "967/967 [==============================] - 39s 40ms/step - loss: 1.0684 - accuracy: 0.6742\n",
            "Epoch 28/50\n",
            "967/967 [==============================] - 39s 40ms/step - loss: 1.0676 - accuracy: 0.6745\n",
            "Epoch 29/50\n",
            "967/967 [==============================] - 39s 40ms/step - loss: 1.0670 - accuracy: 0.6745\n",
            "Epoch 30/50\n",
            "967/967 [==============================] - 39s 40ms/step - loss: 1.0666 - accuracy: 0.6745\n",
            "Epoch 31/50\n",
            "967/967 [==============================] - 39s 40ms/step - loss: 1.0660 - accuracy: 0.6747\n",
            "Epoch 32/50\n",
            "967/967 [==============================] - 39s 40ms/step - loss: 1.0655 - accuracy: 0.6749\n",
            "Epoch 33/50\n",
            "967/967 [==============================] - 39s 40ms/step - loss: 1.0652 - accuracy: 0.6751\n",
            "Epoch 34/50\n",
            "967/967 [==============================] - 39s 40ms/step - loss: 1.0648 - accuracy: 0.6750\n",
            "Epoch 35/50\n",
            "967/967 [==============================] - 39s 40ms/step - loss: 1.0644 - accuracy: 0.6751\n",
            "Epoch 36/50\n",
            "967/967 [==============================] - 39s 40ms/step - loss: 1.0642 - accuracy: 0.6752\n",
            "Epoch 37/50\n",
            "967/967 [==============================] - 39s 40ms/step - loss: 1.0638 - accuracy: 0.6753\n",
            "Epoch 38/50\n",
            "967/967 [==============================] - 39s 40ms/step - loss: 1.0634 - accuracy: 0.6753\n",
            "Epoch 39/50\n",
            "967/967 [==============================] - 39s 40ms/step - loss: 1.0636 - accuracy: 0.6752\n",
            "Epoch 40/50\n",
            "967/967 [==============================] - 39s 40ms/step - loss: 1.0633 - accuracy: 0.6755\n",
            "Epoch 41/50\n",
            "967/967 [==============================] - 39s 40ms/step - loss: 1.0631 - accuracy: 0.6756\n",
            "Epoch 42/50\n",
            "967/967 [==============================] - 39s 40ms/step - loss: 1.0624 - accuracy: 0.6756\n",
            "Epoch 43/50\n",
            "967/967 [==============================] - 39s 40ms/step - loss: 1.0632 - accuracy: 0.6754\n",
            "Epoch 44/50\n",
            "967/967 [==============================] - 39s 40ms/step - loss: 1.0627 - accuracy: 0.6755\n",
            "Epoch 45/50\n",
            "967/967 [==============================] - 39s 40ms/step - loss: 1.0630 - accuracy: 0.6756\n",
            "Epoch 46/50\n",
            "967/967 [==============================] - 39s 40ms/step - loss: 1.0628 - accuracy: 0.6754\n",
            "Epoch 47/50\n",
            "967/967 [==============================] - 39s 40ms/step - loss: 1.0626 - accuracy: 0.6755\n",
            "Epoch 48/50\n",
            "967/967 [==============================] - 39s 40ms/step - loss: 1.0628 - accuracy: 0.6753\n",
            "Epoch 49/50\n",
            "967/967 [==============================] - 39s 40ms/step - loss: 1.0628 - accuracy: 0.6754\n",
            "Epoch 50/50\n",
            "967/967 [==============================] - 39s 40ms/step - loss: 1.0625 - accuracy: 0.6755\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vijz43ktoA_o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "16ce963b-8d06-4086-ac5f-5bc1bef4d2f3"
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "model.load_weights(latest_check)\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "model.summary()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (1, None, 300)            31800     \n",
            "_________________________________________________________________\n",
            "gru_6 (GRU)                  (1, None, 512)            1250304   \n",
            "_________________________________________________________________\n",
            "gru_7 (GRU)                  (1, None, 256)            591360    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (1, None, 106)            27242     \n",
            "=================================================================\n",
            "Total params: 1,900,706\n",
            "Trainable params: 1,900,706\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMJJAO_SoGSG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  num_generate = 1000  #can be anything you like\n",
        "  input_eval = [char2index[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "  text_generated = []\n",
        "  scaling = 0.5 #kept at a lower value here\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    # remove the batch dimension\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "    predictions = predictions / scaling\n",
        "    predicted_id = tf.random.categorical(predictions,num_samples=1)[-1,0].numpy()\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "    text_generated.append(index2char[predicted_id])\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyCkLGiUojvD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "98bc14fa-dc4c-46fd-e937-66c10d7734a5"
      },
      "source": [
        "print(generate_text(model, start_string=\"Severus Snape\"))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Severus Snape was methe they had been thinking about it. The Weasleys wand still in a state. Harry found himself being stuck in the stairs and saw that she could not be able to stop the minutes before the rather stupid black hair creaked away from the little cold and interested in a very little glove. He stood up, and he saw the sound of the place where he looked at each other, the boy who had already stood there with the cart and started sickening as he spoke, saw him so hard that he had been crammed into the castle, then she put her hair again, and she was came to find himself. \"It has happened to you? I am going to be friends... You havenâ€™t been to know where this could just continue my father was a little one of the wand - he had not been happy at all, the distant bunch of green light and he stood up, and Harry saw the locket into the trees. He was so close that they were all still absolute at the same time. He seemed to have been there, Harry thought he was working to the best to mention it hi\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}